# PolicyLens
**Turning complex policies into clear answers.**

PolicyLens is an AI-powered policy assistant that helps users ask natural language questions and receive grounded, context-aware answers from internal policy documents. Built using a Retrieval-Augmented Generation (RAG) architecture, it combines semantic search with large language models to deliver accurate, trustworthy responses.

---

## âœ¨ Key Features

- ğŸ” **Semantic Policy Search** â€“ FAISS vector indexing for fast, relevant retrieval
- ğŸ¤– **LLM-Powered Answers** â€“ Intelligent responses with robust fallback logic
- ğŸ“š **Source Attribution** â€“ Full transparency with cited policy references
- ğŸ§  **Context-Aware Responses** â€“ Answers grounded strictly in policy text
- ğŸ”’ **Secure & Reproducible** â€“ No raw data or secrets in version control

---

## ğŸ§  How It Works

1. **Document Processing** â€“ Policy documents are parsed, cleaned, and split into semantic chunks
2. **Embedding Generation** â€“ Each chunk is converted into a vector embedding
3. **Vector Indexing** â€“ Embeddings are indexed using FAISS for fast similarity search
4. **Query Processing**:
   - User submits a natural language question
   - System retrieves most relevant policy chunks
   - LLM generates response **only from retrieved context**
5. **Response Display**:
   - Clear, concise answer
   - Supporting policy sources
   - Model attribution

---

## ğŸ—ï¸ Architecture Overview
```
User Query
    â†“
Frontend (Next.js)
    â†“
Backend (FastAPI)
    â”œâ”€â”€ FAISS Vector Search
    â”œâ”€â”€ Embedding Model
    â””â”€â”€ LLM (OpenAI/Gemini with fallback)
```

---

## ğŸ§° Tech Stack

### Frontend
- **Next.js** â€“ React framework for production
- **Tailwind CSS** â€“ Utility-first styling

### Backend
- **FastAPI** â€“ High-performance Python API framework
- **FAISS** â€“ Efficient similarity search and clustering
- **OpenAI / Gemini** â€“ LLM providers with automatic fallback

### ML/NLP
- **Text Embedding Models(Sentence Transformers / Gemini / OpenAI)** â€“ Text embeddings
- **RAG Architecture** â€“ Retrieval-Augmented Generation

---

## ğŸ“‚ Repository Structure
```
policy_assistant/
â”‚
â”œâ”€â”€ backend/               # FastAPI backend & logic
â”‚   â”œâ”€â”€ rag/               # RAG specific modules
â”‚   â”œâ”€â”€ main.py            # API entry point
â”‚   â”œâ”€â”€ rag_engine.py      # Core RAG logic integration
â”‚   â”œâ”€â”€ config.py          # Configuration settings
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â”‚
â”œâ”€â”€ frontend/              # Next.js frontend application
â”‚   â”œâ”€â”€ src/               # Source code
â”‚   â”‚   â””â”€â”€ app/           # App router pages & components
â”‚   â”œâ”€â”€ public/            # Static assets
â”‚   â””â”€â”€ package.json       # Node dependencies
â”‚
â”œâ”€â”€ data_processing/       # Data ingestion & processing pipeline
â”‚   â”œâ”€â”€ README.md          # Pipeline documentation
â”‚   â”œâ”€â”€ chunk_splitter.py  # Text chunking logic
â”‚   â”œâ”€â”€ build_faiss_index.py # Vector indexing script
â”‚   â””â”€â”€ query_faiss.py     # Search testing script
â”‚
â”œâ”€â”€ .gitignore             # Git ignore rules
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ“Š Data & Reproducibility

> âš ï¸ **Note:** Raw datasets, embeddings, and FAISS indexes are intentionally excluded from this repository.

**Why?**
- Large file sizes unsuitable for version control
- Security and licensing considerations
- Industry best practices for data management

**Good news:** The entire pipeline is **fully reproducible** using the scripts in `data_processing/`.

ğŸ“„ See **[data_processing/README.md](data_processing/README.md)** for complete instructions.

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 16+
- API keys for OpenAI and/or Google Gemini

### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```

Backend will run on `http://localhost:8000`

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
```

Frontend will run on `http://localhost:3000`

### Environment Variables

Create a `.env` file in the `backend/` directory:
```env
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
```

---

## ğŸ’¡ Use Case Examples

**Sample Questions:**
- "What is the company's termination policy?"
- "How does the remote work policy work?"
- "What are the guidelines for leave and benefits?"
- "Is there a dress code mentioned in the employee handbook?"

PolicyLens provides answers **only when supported by policy context**. If information isn't available in the documents, it clearly states this limitation rather than hallucinating.

---

## ğŸ”® Future Enhancements

- [ ] Multi-document upload support
- [ ] User feedback loop for improving responses
- [ ] Advanced filtering (by department, policy type, date)
- [ ] Conversation history and follow-up questions
- [ ] Export responses with citations

---

## ğŸ“ License

This project is licensed under the MIT License.

---

## ğŸ‘¤ Author

**Lakshya**

Questions or feedback? Feel free to open an issue or reach out!

---

## ğŸ™ Acknowledgments

Built with:
- [FAISS](https://github.com/facebookresearch/faiss) by Facebook Research
- [OpenAI API](https://openai.com/)
- [Google Gemini](https://deepmind.google/technologies/gemini/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Next.js](https://nextjs.org/)